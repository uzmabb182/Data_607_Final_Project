---
title: "Data 607 - Final Project"
output: html_document
---

### Project Proposal:

Source of motivation for the final project: Loan Prediction

NJ Housing Prices dataset: https://www.realtor.com/research/data/

Highest level of educational attainment:	Completing College

 https://data.ers.usda.gov/reports.aspx?ID=17829
 
Web Scraping list of fips code: https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county

API Call to census five years data:

https://api.census.gov/data/2019/acs/acs1?get=NAME,B01001_001E&for=county:*

I am sharing Forbes magazine's article. My source of motivation for this project is to investigate one of the paperâ€™s key findings including:

"Students with a family income of $100,000 or more are more than twice as likely as students with family income under $50,000 to have combined SAT test scores of 1400 to 1600".

I will perform web scraping to fetch SAT data for several years and API call for the census five-year data. Both of my datasets will be for the state of New Jersey counties. I will perform the linear regression and try for other regression to find the correlation between the family income and students' SAT scores.

Also, perform the one of the machine learning to predict the next year's SAT outcome.


Importing Libraries

```{r}
library(tidyverse)
library(dplyr)
library(plotly)
library(tidyr)
library(ggthemes)
library(corrplot)
library(GGally)
library(DT)
library(caret)
library(pROC)
library(randomForest)
library(sqldf)
library(sqldf)
library(DescTools)

```

### Exploratory Data Analysis(EDA):
```{r}
train_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_607_Final_Project/main/2019loans.csv")

head(train_df)
```

```{r}
dim(train_df)
```

```{r}
colnames(train_df)
```

```{r}
table(train_df['loan_status'])
```

### Droping Null Columns

```{r}
train_df <- train_df %>% discard(~all(is.na(.) | . ==""))

head(train_df)
```

### Using na.omit() to remove (missing) NA and NaN values

```{r}
train_df <- na.omit(train_df) 
head(train_df)
```

### Drop the unwanted columns from train_df

```{r}
# Drop the columns of the dataframe
train_df <- select (train_df,-c(X1,index))
head(train_df)
```



```{r}
test_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_607_Final_Project/main/2020Q1loans.csv")

head(test_df)
```
```{r}
dim(test_df)
```

```{r}
table(test_df['loan_status'])
```

### Droping Null Columns

```{r}
test_df <- test_df %>% discard(~all(is.na(.) | . ==""))

head(test_df)
```

### Using na.omit() to remove (missing) NA and NaN values

```{r}
test_df <- na.omit(test_df) 
head(test_df)
```

### Drop the unwanted columns from train_df

```{r}
# Drop the columns of the dataframe
test_df <- select (test_df,-c(X1,index))
head(test_df)
```

### Loan Data Visualization:
### The distribution of loan amounts shows that a majority of the loan amount are between $5,000 and $25,000

```{r}

ggplot(data=train_df, aes(loan_amnt))+geom_histogram(bins = 40,color="blue",fill="purple")

```

### Showing the spike at the maximum loan amount

```{r}
Desc(train_df$loan_amnt, main = "Loan Amount Distribution", plotit = TRUE)
```

### Analyzing the annual income of the borrower of the loans

```{r}
Desc(train_df$annual_inc, main = "Annual Income", plotit = TRUE)
```


```{r}
ggplot(train_df, aes(x=verification_status, y=loan_amnt, fill=verification_status)) +
  stat_summary(fun.y="sum", geom="bar") +
  labs(y ="Total Loan Amount",title="Total loan amount Vs Verification_Status")

```
```{r}
ggplot(train_df, aes(x=loan_status, y=loan_amnt, fill=loan_status)) +
  stat_summary(fun.y="sum", geom="bar") +
  labs(y ="Total Loan Amount",title="Total loan amount Vs loan_status")

```

### summarize the class distribution

```{r}
percentage <- prop.table(table(train_df$loan_status)) * 100
cbind(freq=table(train_df$loan_status), percentage=percentage)
```




### Preprocessing Data

```{r}
# # Create our target-2019
y_train_df <-  train_df$loan_status

```

```{r}
cardio_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_607_Final_Project/main/cardiovascular_disease.csv")

head(cardio_df)
```



```{r}
library(caret)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(cardio_df$cardio, p=0.80, list=FALSE)
```



```{r}
# select 20% of the data for validation
validation <- cardio_df[-validation_index,]
```


```{r}
# use the remaining 80% of data to training and testing the models
cardio_df <- cardio_df[validation_index,]
```

```{r}
# dimensions of dataset
dim(cardio_df)
```

```{r}
# list types for each attribute
sapply(cardio_df, class)
```

```{r}
head(cardio_df)
```

```{r}
# list the levels for the class
levels(cardio_df$cardio)
```

